# Mamba: Linear-Time Sequence Modeling with Selective State Spaces

## Official Paper and GitHub Repository
- **Title:** [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/abs/2312.00752)
- **GitHub Repository:** [state-spaces/mamba](https://github.com/state-spaces/mamba)

## Hugging Face Resources
- **Blog Post:** [Get on the SSM Train](https://huggingface.co/blog/lbourdois/get-on-the-ssm-train)
- **Mamba in Hugging Face Transformers:** [Model Documentation](https://huggingface.co/docs/transformers/en/model_doc/mamba)

## Videos and Explainers
- **Mamba: A New State Space Model for NLP and Beyond:**  
  - [YouTube Video 1](https://www.youtube.com/watch?v=oSv0Z3fm6os)  
  - [YouTube Video 2](https://youtu.be/8Q_tqwpTpVU?si=I1XkOw86bnCtneR3)

- **Explaining Mamba: A Faster Alternative to Transformers?**  
  - [Papers with Code](https://paperswithcode.com/paper/mamba-linear-time-sequence-modeling-with)

- **State Space Models (SSMs) Overview:**  
  - [YouTube Video](https://youtu.be/OpJMn8T7Z34?si=S6rp1EVHy_mFKa-D)

## Additional Resources
- **Mamba GitHub Repository:** [GitHub](https://github.com/state-spaces/mamba)
